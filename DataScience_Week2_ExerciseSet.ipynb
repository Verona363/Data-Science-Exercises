{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc96HGV0pYkJ"
      },
      "source": [
        "# Introduction to Data Science 2025\n",
        "\n",
        "# Week 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWh-qfRJpYkK"
      },
      "source": [
        "## Exercise 1 | Titanic: data preprocessing and imputation\n",
        "<span style=\"font-weight: bold\"> *Note: You can find tutorials for NumPy and Pandas under 'Useful tutorials' in the course material.*</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZy31vJxpYkK"
      },
      "source": [
        "Download the [Titanic dataset](https://www.kaggle.com/c/titanic) [train.csv] from Kaggle or <span style=\"font-weight: 500\">directly from the course material</span>, and complete the following exercises. If you choose to download the dataset from Kaggle, you will need to create a Kaggle account unless you already have one, but it is quite straightforward.\n",
        "\n",
        "The dataset consists of personal information of all the passengers on board the RMS Titanic, along with information about whether they survived the iceberg collision or not.\n",
        "\n",
        "1. Your first task is to read the data file and print the shape of the data.\n",
        "\n",
        "    <span style=\"font-weight: 500\"> *Hint 1: You can read them into a Pandas dataframe if you wish.*</span>\n",
        "    \n",
        "    <span style=\"font-weight: 500\"> *Hint 2: The shape of the data should be (891, 12).*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "l4kCNDespYkK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   PassengerId  Survived  Pclass  \\\n",
            "0            1         0       3   \n",
            "1            2         1       1   \n",
            "2            3         1       3   \n",
            "3            4         1       1   \n",
            "4            5         0       3   \n",
            "\n",
            "                                                Name     Sex   Age  SibSp  \\\n",
            "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
            "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
            "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
            "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
            "4                           Allen, Mr. William Henry    male  35.0      0   \n",
            "\n",
            "   Parch            Ticket     Fare Cabin Embarked  \n",
            "0      0         A/5 21171   7.2500   NaN        S  \n",
            "1      0          PC 17599  71.2833   C85        C  \n",
            "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
            "3      0            113803  53.1000  C123        S  \n",
            "4      0            373450   8.0500   NaN        S  \n",
            "(891, 12)\n"
          ]
        }
      ],
      "source": [
        "#\"C:\\Users\\HP LAPTOP 15S-FQ2023\\titanic_data\"\n",
        "import pandas as pd\n",
        "\n",
        "# Use the full path to your CSV file\n",
        "train = pd.read_csv(r\"C:\\Users\\HP LAPTOP 15S-FQ2023\\titanic_data\\train.csv\")\n",
        "train_original=train.copy\n",
        "\n",
        "\n",
        "# Show the first 5 rows to confirm it loaded correctly\n",
        "print(train.head())\n",
        "print(train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCZoOS-wpYkL"
      },
      "source": [
        "2. Let's look at the data and get started with some preprocessing. Some of the columns, e.g <span style=\"font-weight: 500\"> *Name*</span>, simply identify a person and are not useful for prediction tasks. Try to identify these columns, and remove them.\n",
        "\n",
        "    <span style=\"font-weight: 500\"> *Hint: The shape of the data should now be (891, 9).*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "m18V-jpTpYkL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     Survived  Pclass     Sex   Age  SibSp  Parch     Fare Cabin Embarked\n",
            "0           0       3    male  22.0      1      0   7.2500   NaN        S\n",
            "1           1       1  female  38.0      1      0  71.2833   C85        C\n",
            "2           1       3  female  26.0      0      0   7.9250   NaN        S\n",
            "3           1       1  female  35.0      1      0  53.1000  C123        S\n",
            "4           0       3    male  35.0      0      0   8.0500   NaN        S\n",
            "..        ...     ...     ...   ...    ...    ...      ...   ...      ...\n",
            "886         0       2    male  27.0      0      0  13.0000   NaN        S\n",
            "887         1       1  female  19.0      0      0  30.0000   B42        S\n",
            "888         0       3  female   NaN      1      2  23.4500   NaN        S\n",
            "889         1       1    male  26.0      0      0  30.0000  C148        C\n",
            "890         0       3    male  32.0      0      0   7.7500   NaN        Q\n",
            "\n",
            "[891 rows x 9 columns]\n",
            "(891, 9)\n"
          ]
        }
      ],
      "source": [
        "train=train.drop([\"PassengerId\", \"Name\", \"Ticket\"], axis=1)\n",
        "print(train)\n",
        "print(train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48AcPDOrpYkL"
      },
      "source": [
        "3. The column <span style=\"font-weight: 500\">*Cabin*</span> contains a letter and a number. A smart catch at this point would be to notice that the letter stands for the deck level on the ship. Keeping just the deck information would be more informative when developing, e.g. a classifier that predicts whether a passenger survived. The next step in our preprocessing will be to add a new column to the dataset, which consists simply of the deck letter. You can then remove the original <span style=\"font-weight: 500\">*Cabin*</span>-column.\n",
        "\n",
        "<span style=\"font-weight: 500\">*Hint: The deck letters should be ['A' 'B' 'C' 'D' 'E' 'F' 'G' 'T'].*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "OQX5LmWrpYkL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nan 'C' 'E' 'G' 'D' 'A' 'B' 'F' 'T']\n",
            "(891, 9)\n"
          ]
        }
      ],
      "source": [
        "train[\"Deck\"]=train[\"Cabin\"].str[0]\n",
        "train=train.drop(\"Cabin\", axis=1)\n",
        "print(train[\"Deck\"].unique())\n",
        "print(train.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mupSH5tXpYkL"
      },
      "source": [
        "4. You’ll notice that some of the columns, such as the previously added deck number, are [categorical](https://en.wikipedia.org/wiki/Categorical_variable). To preprocess the categorical variables so that they're ready for further computation, we need to avoid the current string format of the values. This means the next step for each categorical variable is to transform the string values to numeric ones, that correspond to a unique integer ID representative of each distinct category. This process is called label encoding and you can read more about it [here](https://pandas.pydata.org/docs/user_guide/categorical.html).\n",
        "\n",
        "    <span style=\"font-weight: 500\">*Hint: Pandas can do this for you.*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfb0_uzRpYkL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Embarked  Deck\n",
            "0         0       3    1  22.0      1      0   7.2500         2    -1\n",
            "1         1       1    0  38.0      1      0  71.2833         0     2\n",
            "2         1       3    0  26.0      0      0   7.9250         2    -1\n",
            "3         1       1    0  35.0      1      0  53.1000         2     2\n",
            "4         0       3    1  35.0      0      0   8.0500         2    -1\n"
          ]
        }
      ],
      "source": [
        "for col in [\"Sex\", \"Embarked\", \"Deck\"]:\n",
        "    train[col]=train[col].astype(\"category\").cat.codes\n",
        "\n",
        "print(train.head())\n",
        "#sex: 1:male, 2:female\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32_hHxGLpYkL"
      },
      "source": [
        "5. Next, let's look into missing value **imputation**. Some of the rows in the data have missing values, e.g when the cabin number of a person is unknown. Most machine learning algorithms have trouble with missing values, and they need to be handled during preprocessing:\n",
        "\n",
        "    a) For continuous variables, replace the missing values with the mean of the non-missing values of that column.\n",
        "\n",
        "    b) For categorical variables, replace the missing values with the mode of the column.\n",
        "\n",
        "    <span style=\"font-weight: 500\">*Remember: Even though in the previous step we transformed categorical variables into their numeric representation, they are still categorical.*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "e0kh2bbGpYkL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Survived    0\n",
            "Pclass      0\n",
            "Sex         0\n",
            "Age         0\n",
            "SibSp       0\n",
            "Parch       0\n",
            "Fare        0\n",
            "Embarked    0\n",
            "Deck        0\n",
            "dtype: int64\n",
            "Survived    0\n",
            "Pclass      0\n",
            "Sex         0\n",
            "Age         0\n",
            "SibSp       0\n",
            "Parch       0\n",
            "Fare        0\n",
            "Embarked    0\n",
            "Deck        0\n",
            "dtype: int64\n",
            "   Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Embarked  Deck\n",
            "0         0       3    1  22.0      1      0   7.2500         2    -1\n",
            "1         1       1    0  38.0      1      0  71.2833         0     2\n",
            "2         1       3    0  26.0      0      0   7.9250         2    -1\n",
            "3         1       1    0  35.0      1      0  53.1000         2     2\n",
            "4         0       3    1  35.0      0      0   8.0500         2    -1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\HP LAPTOP 15S-FQ2023\\AppData\\Local\\Temp\\ipykernel_34748\\3278853669.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train[\"Age\"].fillna(train[\"Age\"].mean(), inplace=True)\n",
            "C:\\Users\\HP LAPTOP 15S-FQ2023\\AppData\\Local\\Temp\\ipykernel_34748\\3278853669.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train[\"Embarked\"].fillna(train[\"Embarked\"].mode()[0], inplace=True)\n",
            "C:\\Users\\HP LAPTOP 15S-FQ2023\\AppData\\Local\\Temp\\ipykernel_34748\\3278853669.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  train[\"Deck\"].fillna(train[\"Deck\"].mode()[0], inplace=True)\n"
          ]
        }
      ],
      "source": [
        "#first, identify the coloumns that have missing data\n",
        "print(train.isnull().sum())\n",
        "#age\n",
        "train[\"Age\"].fillna(train[\"Age\"].mean(), inplace=True)\n",
        "train[\"Embarked\"].fillna(train[\"Embarked\"].mode()[0], inplace=True)\n",
        "train[\"Deck\"].fillna(train[\"Deck\"].mode()[0], inplace=True)\n",
        "print(train.isnull().sum())\n",
        "print(train.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15Kbmgx9pYkM"
      },
      "source": [
        "6. At this point, all data is numeric. Write the data, with the modifications we made, to a  <span style=\"font-weight: 500\"> .csv</span> file. Then, write another file, this time in <span style=\"font-weight: 500\">JSON</span> format, with the following structure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "J_EhC78NpYkM"
      },
      "outputs": [],
      "source": [
        "#[\n",
        "#    {\n",
        "#        \"Deck\": 0,\n",
        "#        \"Age\": 20,\n",
        "#        \"Survived\", 0\n",
        "#        ...\n",
        "#    },\n",
        "#    {\n",
        "#        ...\n",
        "#    }\n",
        "#]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "lxF-ehbapYkM"
      },
      "outputs": [],
      "source": [
        "train.to_csv(r\"C:\\Users\\HP LAPTOP 15S-FQ2023\\titanic_data\\train_cleaned.csv\", index=False)\n",
        "#index=False ensures the row numbers are not included in the file.\n",
        "#JSON\n",
        "#\"records\" → list of dictionaries (good for row-wise data)\n",
        "#\"index\" → dictionary of dictionaries keyed by index\n",
        "#\"columns\" → dictionary of lists keyed by column name\n",
        "train.to_json(r\"C:\\Users\\HP LAPTOP 15S-FQ2023\\titanic_data\\train_cleaned.json\",\n",
        "               orient=\"records\") # # lines=True gives one JSON object per line\n",
        "\n",
        "#CSV file\n",
        "# Survived,Pclass,Sex,Age,SibSp,Parch,Fare,Embarked,Deck\n",
        "# 0,3,1,22.0,1,0,7.25,2,4\n",
        "# 1,1,0,38.0,1,0,71.28,0,2\n",
        "# 1,3,0,26.0,0,0,7.92,2,3\n",
        "\n",
        "#JSON output with lines=True #not a list of dicitonaries(Each row is a separate JSON object on a new line.) \n",
        "# {\"Survived\":0,\"Pclass\":3,\"Sex\":1,\"Age\":22.0,\"SibSp\":1,\"Parch\":0,\"Fare\":7.25,\"Embarked\":2,\"Deck\":4}\n",
        "# {\"Survived\":1,\"Pclass\":1,\"Sex\":0,\"Age\":38.0,\"SibSp\":1,\"Parch\":0,\"Fare\":71.28,\"Embarked\":0,\"Deck\":2}\n",
        "# {\"Survived\":1,\"Pclass\":3,\"Sex\":0,\"Age\":26.0,\"SibSp\":0,\"Parch\":0,\"Fare\":7.92,\"Embarked\":2,\"Deck\":3}\n",
        "\n",
        "#JSON output without lines=True, (proper JSON list of dicitonaries, can be directly loaded with json.load())\n",
        "# [\n",
        "#   {\"Survived\":0,\"Pclass\":3,\"Sex\":1,\"Age\":22.0,\"SibSp\":1,\"Parch\":0,\"Fare\":7.25,\"Embarked\":2,\"Deck\":4},\n",
        "#   {\"Survived\":1,\"Pclass\":1,\"Sex\":0,\"Age\":38.0,\"SibSp\":1,\"Parch\":0,\"Fare\":71.28,\"Embarked\":0,\"Deck\":2},\n",
        "#   {\"Survived\":1,\"Pclass\":3,\"Sex\":0,\"Age\":26.0,\"SibSp\":0,\"Parch\":0,\"Fare\":7.92,\"Embarked\":2,\"Deck\":3}\n",
        "# ]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnkAgzHjpYkM"
      },
      "source": [
        "Study the records and try to see if there is any evident pattern in terms of chances of survival."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddqs0UqLpYkM"
      },
      "source": [
        "**Remember to submit your code on the MOOC platform. You can return this Jupyter notebook (.ipynb) or .py, .R, etc depending on your programming preferences.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGnzPePKpYkM"
      },
      "source": [
        "## Exercise 2 | Titanic 2.0: exploratory data analysis\n",
        "\n",
        "In this exercise, we’ll continue to study the Titanic dataset from the last exercise. Now that we have done some preprocessing, it’s time to look at the data with some exploratory data analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5nTB9ExpYkM"
      },
      "source": [
        "1. First investigate each feature variable in turn. For each categorical variable, find out the mode, i.e., the most frequent value. For numerical variables, calculate the median value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "hKdMpHZApYkM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Most frequent Survived: 0\n",
            "Most frequent Pclass: 3\n",
            "Most frequent Sex: 1\n",
            "Median Age: 29.69911764705882\n",
            "Number of siblings aboard: 0.0\n",
            "Number of Parents aboard: 0.0\n",
            "Median Fare: 14.4542\n",
            "Most frequent Embarked: 2\n",
            "Most frequent Deck: -1\n"
          ]
        }
      ],
      "source": [
        "survived_mode=train[\"Survived\"].mode()[0]\n",
        "print(\"Most frequent Survived:\", survived_mode)\n",
        "pclass_mode=train[\"Pclass\"].mode()[0]\n",
        "print(\"Most frequent Pclass:\", pclass_mode)\n",
        "sex_mode=train[\"Sex\"].mode()[0]\n",
        "print(\"Most frequent Sex:\", sex_mode)\n",
        "age_median=train[\"Age\"].median()\n",
        "print(\"Median Age:\", age_median)\n",
        "sibsp_median=train[\"SibSp\"].median()\n",
        "print(\"Number of siblings aboard:\", sibsp_median)\n",
        "\n",
        "parch_median=train[\"Parch\"].median()\n",
        "print(\"Number of Parents aboard:\", parch_median)\n",
        "\n",
        "fare_median=train[\"Fare\"].median()\n",
        "print(\"Median Fare:\", fare_median)\n",
        "\n",
        "embarked_mode=train[\"Embarked\"].mode()[0]\n",
        "print(\"Most frequent Embarked:\", embarked_mode)\n",
        "\n",
        "deck_mode=train[\"Deck\"].mode()[0]\n",
        "print(\"Most frequent Deck:\", deck_mode)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOjLqfDkpYkM"
      },
      "source": [
        "2. Next, combine the modes of the categorical variables, and the medians of the numerical variables, to construct an imaginary “average survivor”. This \"average survivor\" should represent the typical passenger of the class of passengers who survived. Also following the same principle, construct the “average non-survivor”.\n",
        "\n",
        "    <span style=\"font-weight: 500\">*Hint 1: What are the average/most frequent variable values for a non-survivor?*</span>\n",
        "    \n",
        "    <span style=\"font-weight: 500\">*Hint 2: You can split the dataframe in two: one subset containing all the survivors and one consisting of all the non-survivor instances. Then, you can use the summary statistics of each of these dataframe to create a prototype \"average survivor\" and \"average non-survivor\", respectively.*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "OhUU2GIDpYkM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Survivor:\n",
            " Pclass       1.000000\n",
            "Sex          0.000000\n",
            "Embarked     2.000000\n",
            "Deck        -1.000000\n",
            "Age         29.699118\n",
            "SibSp        0.000000\n",
            "Parch        0.000000\n",
            "Fare        26.000000\n",
            "dtype: float64\n",
            "Average non_survivor:\n",
            " Pclass       3.000000\n",
            "Sex          1.000000\n",
            "Embarked     2.000000\n",
            "Deck        -1.000000\n",
            "Age         29.699118\n",
            "SibSp        0.000000\n",
            "Parch        0.000000\n",
            "Fare        10.500000\n",
            "dtype: float64\n",
            "              Pclass  Sex  Embarked  Deck        Age  SibSp  Parch  Fare\n",
            "Survivor         1.0  0.0       2.0  -1.0  29.699118    0.0    0.0  26.0\n",
            "Non-Survivor     3.0  1.0       2.0  -1.0  29.699118    0.0    0.0  10.5\n"
          ]
        }
      ],
      "source": [
        "#split the dataset\n",
        "\n",
        "survivors=train[train[\"Survived\"]==1]\n",
        "#selecting rows, creating the condition(Boolean series)\n",
        "non_survivors=train[train[\"Survived\"]==0]\n",
        "\n",
        "numerical_cols = [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]  # adjust based on your dataset\n",
        "survivor_medians = survivors[numerical_cols].median()\n",
        "\n",
        "\n",
        "\n",
        "categorical_cols=[\"Pclass\", \"Sex\", \"Embarked\", \"Deck\"]\n",
        "survivor_modes=survivors[categorical_cols].mode().iloc[0]\n",
        "\n",
        "#non_survivors\n",
        "non_survivor_medians = non_survivors[numerical_cols].median()\n",
        "non_survivor_modes = non_survivors[categorical_cols].mode().iloc[0]\n",
        "\n",
        "average_survivor = pd.concat([survivor_modes, survivor_medians])\n",
        "#combining 2 series of modes and medians\n",
        "#result is a pandas series\n",
        "print(\"Average Survivor:\\n\", average_survivor)\n",
        "\n",
        "average_non_survivor=pd.concat([non_survivor_modes, non_survivor_medians])\n",
        "print(\"Average non_survivor:\\n\", average_non_survivor)\n",
        "\n",
        "average_passengers = pd.DataFrame([average_survivor, average_non_survivor], index=[\"Survivor\", \"Non-Survivor\"])\n",
        "print(average_passengers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3Bax_hlpYkM"
      },
      "source": [
        "3. Next, let's study the distributions of the variables in the two groups (survivor/non-survivor). How well do the average cases represent the respective groups? Can you find actual passengers that are very similar to the (average) representative of their own group? Can you find passengers that are very similar to the (average) representative of the other group?\n",
        "\n",
        "    <span style=\"font-weight: 500\">*Note: Feel free to choose EDA methods according to your preference: non-graphical/graphical, static/interactive - anything goes.*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "mNsSMXRMpYkM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Survivors summary:\n",
            "        Survived      Pclass    Sex         Age       SibSp       Parch  \\\n",
            "count      342.0  342.000000  342.0  342.000000  342.000000  342.000000   \n",
            "unique       NaN         NaN    2.0         NaN         NaN         NaN   \n",
            "top          NaN         NaN    0.0         NaN         NaN         NaN   \n",
            "freq         NaN         NaN  233.0         NaN         NaN         NaN   \n",
            "mean         1.0    1.950292    NaN   28.549778    0.473684    0.464912   \n",
            "std          0.0    0.863321    NaN   13.772498    0.708688    0.771712   \n",
            "min          1.0    1.000000    NaN    0.420000    0.000000    0.000000   \n",
            "25%          1.0    1.000000    NaN   21.000000    0.000000    0.000000   \n",
            "50%          1.0    2.000000    NaN   29.699118    0.000000    0.000000   \n",
            "75%          1.0    3.000000    NaN   35.000000    1.000000    1.000000   \n",
            "max          1.0    3.000000    NaN   80.000000    4.000000    5.000000   \n",
            "\n",
            "              Fare  Embarked   Deck  \n",
            "count   342.000000     342.0  342.0  \n",
            "unique         NaN       4.0    8.0  \n",
            "top            NaN       2.0   -1.0  \n",
            "freq           NaN     217.0  206.0  \n",
            "mean     48.395408       NaN    NaN  \n",
            "std      66.596998       NaN    NaN  \n",
            "min       0.000000       NaN    NaN  \n",
            "25%      12.475000       NaN    NaN  \n",
            "50%      26.000000       NaN    NaN  \n",
            "75%      57.000000       NaN    NaN  \n",
            "max     512.329200       NaN    NaN  \n",
            "\n",
            "Non-survivors summary:\n",
            "        Survived      Pclass    Sex         Age       SibSp       Parch  \\\n",
            "count      549.0  549.000000  549.0  549.000000  549.000000  549.000000   \n",
            "unique       NaN         NaN    2.0         NaN         NaN         NaN   \n",
            "top          NaN         NaN    1.0         NaN         NaN         NaN   \n",
            "freq         NaN         NaN  468.0         NaN         NaN         NaN   \n",
            "mean         0.0    2.531876    NaN   30.415100    0.553734    0.329690   \n",
            "std          0.0    0.735805    NaN   12.457370    1.288399    0.823166   \n",
            "min          0.0    1.000000    NaN    1.000000    0.000000    0.000000   \n",
            "25%          0.0    2.000000    NaN   23.000000    0.000000    0.000000   \n",
            "50%          0.0    3.000000    NaN   29.699118    0.000000    0.000000   \n",
            "75%          0.0    3.000000    NaN   35.000000    1.000000    0.000000   \n",
            "max          0.0    3.000000    NaN   74.000000    8.000000    6.000000   \n",
            "\n",
            "              Fare  Embarked   Deck  \n",
            "count   549.000000     549.0  549.0  \n",
            "unique         NaN       3.0    9.0  \n",
            "top            NaN       2.0   -1.0  \n",
            "freq           NaN     427.0  481.0  \n",
            "mean     22.117887       NaN    NaN  \n",
            "std      31.388207       NaN    NaN  \n",
            "min       0.000000       NaN    NaN  \n",
            "25%       7.854200       NaN    NaN  \n",
            "50%      10.500000       NaN    NaN  \n",
            "75%      26.000000       NaN    NaN  \n",
            "max     263.000000       NaN    NaN  \n",
            "Survivors - Sex distribution:\n",
            " Sex\n",
            "0    233\n",
            "1    109\n",
            "Name: count, dtype: int64\n",
            "Non-survivors - Sex distribution:\n",
            " Sex\n",
            "1    468\n",
            "0     81\n",
            "Name: count, dtype: int64\n",
            "Survivors - Embarked distribution:\n",
            " Embarked\n",
            " 2    217\n",
            " 0     93\n",
            " 1     30\n",
            "-1      2\n",
            "Name: count, dtype: int64\n",
            "Non-survivors - Embarked distribution:\n",
            " Embarked\n",
            " 2    427\n",
            " 0     75\n",
            " 1     47\n",
            "-1      0\n",
            "Name: count, dtype: int64\n",
            "Survivors - Deck distribution:\n",
            " Deck\n",
            "-1    206\n",
            " 1     35\n",
            " 2     35\n",
            " 3     25\n",
            " 4     24\n",
            " 5      8\n",
            " 0      7\n",
            " 6      2\n",
            " 7      0\n",
            "Name: count, dtype: int64\n",
            "Non-survivors - Deck distribution:\n",
            " Deck\n",
            "-1    481\n",
            " 2     24\n",
            " 1     12\n",
            " 0      8\n",
            " 3      8\n",
            " 4      8\n",
            " 5      5\n",
            " 6      2\n",
            " 7      1\n",
            "Name: count, dtype: int64\n",
            "Closest passenger to average survivor:\n",
            " Survived     1.000000\n",
            "Pclass       1.000000\n",
            "Sex          1.000000\n",
            "Age         29.699118\n",
            "SibSp        0.000000\n",
            "Parch        0.000000\n",
            "Fare        26.550000\n",
            "Embarked     2.000000\n",
            "Deck        -1.000000\n",
            "distance     0.550000\n",
            "Name: 507, dtype: float64\n",
            "Closest passenger to non_survivor Survived     0.000000\n",
            "Pclass       2.000000\n",
            "Sex          1.000000\n",
            "Age         30.000000\n",
            "SibSp        0.000000\n",
            "Parch        0.000000\n",
            "Fare        10.500000\n",
            "Embarked     2.000000\n",
            "Deck        -1.000000\n",
            "distance     0.300882\n",
            "Name: 219, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Convert categorical columns first\n",
        "for col in [\"Sex\", \"Embarked\", \"Deck\"]:\n",
        "    train[col] = train[col].astype(\"category\")\n",
        "\n",
        "# Now create subsets\n",
        "survivors = train[train[\"Survived\"] == 1].copy()\n",
        "non_survivors = train[train[\"Survived\"] == 0].copy()\n",
        "\n",
        "# Now describe\n",
        "#Survivors\n",
        "print(\"Survivors summary:\")\n",
        "print(survivors.describe(include=\"all\"))\n",
        "\n",
        "#Non-survivors\n",
        "print(\"\\nNon-survivors summary:\")\n",
        "print(non_survivors.describe(include=\"all\"))\n",
        "\n",
        "#Counts how many times instance from each category appears\n",
        "for col in [\"Sex\", \"Embarked\", \"Deck\"]:\n",
        "    print(f\"Survivors - {col} distribution:\\n\", survivors[col].value_counts())\n",
        "    print(f\"Non-survivors - {col} distribution:\\n\", non_survivors[col].value_counts())\n",
        "\n",
        "#For a more visual check\n",
        "# add plots histograms/boxplots for numeric\n",
        "# bar charts for categorical\n",
        "#CHECK THIS CODE FOR VISUL LATER ON\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# for col in [\"Age\", \"Fare\", \"SibSp\", \"Parch\"]:\n",
        "#     plt.figure()\n",
        "#     train_copy.groupby(\"Survived\")[col].plot(kind=\"kde\", legend=True)\n",
        "#     plt.title(f\"Distribution of {col} by survival\")\n",
        "#     plt.show()\n",
        "\n",
        "# for col in [\"Sex\", \"Embarked\", \"Pclass\", \"Deck\"]:\n",
        "#     print(f\"\\n{col} distribution by survival:\")\n",
        "#     print(train_copy.groupby(\"Survived\")[col].value_counts(normalize=True))\n",
        "\n",
        "    import numpy as np\n",
        "\n",
        "numerical_cols=[\"Age\", \"Fare\", \"SibSp\", \"Parch\"]\n",
        "categorical_cols=[\"Sex\", \"Embarked\", \"Pclass\", \"Deck\"]\n",
        "\n",
        "# Compute distances for survivors\n",
        "\n",
        "survivors[\"distance\"]=np.linalg.norm(\n",
        "    survivors[numerical_cols]-average_survivor[numerical_cols], axis=1)\n",
        "\n",
        "#Explanation\n",
        "# survivors[numerical_cols]: a DataFrame with just the numeric columns (Age, Fare, SibSp, Parch) for all survivors\n",
        "# Example shape: (342, 4).\n",
        "# average_survivor[numerical_cols]: a Series containing the median values of those numeric columns for survivors (one \"average\" case) (go up to part b if you dont understand)\n",
        "# np.linalg.norm(..., axis=1) → computes the Euclidean distance (L2 norm) of each row’s difference vector\n",
        "#for the Euclian distance it makes sense to work only with numeric coloumns\n",
        "# axis=1 means: compute across columns, so you get one distance per row (passenger)\n",
        "# Result: a 1D NumPy array with the same length as the number of survivors\n",
        "#Finally, we assign that to a new column: survivors[\"distance\"]\n",
        "\n",
        "closest_survivor=survivors.loc[survivors[\"distance\"].idxmin()]\n",
        "print(\"Closest passenger to average survivor:\\n\", closest_survivor)\n",
        "\n",
        "# survivors[\"distance\"].idxmin(): finds the index (row label) of the passenger with the smallest distance\n",
        "# (i.e., the passenger numerically most similar to the average survivor).\n",
        "# .loc[...] → retrieves that exact row from the DataFrame.\n",
        "# closest_survivor is a single row (Series) representing the real passenger closest to the \"average survivor\".\n",
        "\n",
        "non_survivors[\"distance\"]=np.linalg.norm(\n",
        "    non_survivors[numerical_cols]-average_non_survivor[numerical_cols], axis=1\n",
        ")\n",
        "\n",
        "closest_non_survivor=non_survivors.loc[non_survivors[\"distance\"].idxmin()]\n",
        "print(\"Closest passenger to non_survivor\", closest_non_survivor)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Survivors - Sex distribution:\n",
            " Sex\n",
            "0    233\n",
            "1    109\n",
            "Name: count, dtype: int64\n",
            "Non-survivors - Sex distribution:\n",
            " Sex\n",
            "1    468\n",
            "0     81\n",
            "Name: count, dtype: int64\n",
            "Survivors - Embarked distribution:\n",
            " Embarked\n",
            " 2    217\n",
            " 0     93\n",
            " 1     30\n",
            "-1      2\n",
            "Name: count, dtype: int64\n",
            "Non-survivors - Embarked distribution:\n",
            " Embarked\n",
            " 2    427\n",
            " 0     75\n",
            " 1     47\n",
            "-1      0\n",
            "Name: count, dtype: int64\n",
            "Survivors - Deck distribution:\n",
            " Deck\n",
            "-1    206\n",
            " 1     35\n",
            " 2     35\n",
            " 3     25\n",
            " 4     24\n",
            " 5      8\n",
            " 0      7\n",
            " 6      2\n",
            " 7      0\n",
            "Name: count, dtype: int64\n",
            "Non-survivors - Deck distribution:\n",
            " Deck\n",
            "-1    481\n",
            " 2     24\n",
            " 1     12\n",
            " 0      8\n",
            " 3      8\n",
            " 4      8\n",
            " 5      5\n",
            " 6      2\n",
            " 7      1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "for col in [\"Sex\", \"Embarked\", \"Deck\"]:\n",
        "    print(f\"Survivors - {col} distribution:\\n\", survivors[col].value_counts())\n",
        "    print(f\"Non-survivors - {col} distribution:\\n\", non_survivors[col].value_counts())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Most typical survivor:\n",
            " Survived     1.000000\n",
            "Pclass       1.000000\n",
            "Sex          1.000000\n",
            "Age         29.699118\n",
            "SibSp        0.000000\n",
            "Parch        0.000000\n",
            "Fare        26.550000\n",
            "Embarked     2.000000\n",
            "Deck        -1.000000\n",
            "Name: 507, dtype: float64\n",
            "Most typical non-survivor:\n",
            " Survived     0.000000\n",
            "Pclass       3.000000\n",
            "Sex          1.000000\n",
            "Age         29.699118\n",
            "SibSp        0.000000\n",
            "Parch        0.000000\n",
            "Fare         9.500000\n",
            "Embarked     2.000000\n",
            "Deck        -1.000000\n",
            "Name: 868, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# For survivors\n",
        "numeric_and_categorical_cols = numerical_cols + categorical_cols\n",
        "\n",
        "# Distance from average survivor\n",
        "distances_to_survivor_avg = survivors[numeric_and_categorical_cols].apply(\n",
        "    lambda row: np.linalg.norm(row - average_survivor[numeric_and_categorical_cols]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Smallest distance = most similar passenger\n",
        "most_typical_survivor = survivors.loc[distances_to_survivor_avg.idxmin()]\n",
        "print(\"Most typical survivor:\\n\", most_typical_survivor)\n",
        "\n",
        "distances_to_non_survivor_avg = non_survivors[numeric_and_categorical_cols].apply(\n",
        "    lambda row: np.linalg.norm(row - average_non_survivor[numeric_and_categorical_cols]),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "most_typical_non_survivor = non_survivors.loc[distances_to_non_survivor_avg.idxmin()]\n",
        "print(\"Most typical non-survivor:\\n\", most_typical_non_survivor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Survivor most similar to non-survivor average:\n",
            " Survived     1.0\n",
            "Pclass       3.0\n",
            "Sex          1.0\n",
            "Age         30.0\n",
            "SibSp        0.0\n",
            "Parch        0.0\n",
            "Fare         9.5\n",
            "Embarked     2.0\n",
            "Deck        -1.0\n",
            "Name: 286, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "#find survivor closest to non_survivor average\n",
        "# dist_survivor_to_non_avg = survivors[numeric_and_categorical_cols].apply(\n",
        "#     lambda row: np.linalg.norm(row - average_non_survivor[numeric_and_categorical_cols]),\n",
        "#     axis=1\n",
        "# )\n",
        "# closest_survivor_to_non = survivors.loc[dist_survivor_to_non_avg.idxmin()]\n",
        "# print(\"Survivor most similar to non-survivor average:\\n\", closest_survivor_to_non)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closest passenger to average survivor:\n",
            " Survived     1.000000\n",
            "Pclass       1.000000\n",
            "Sex          1.000000\n",
            "Age         29.699118\n",
            "SibSp        0.000000\n",
            "Parch        0.000000\n",
            "Fare        26.550000\n",
            "Embarked     2.000000\n",
            "Deck        -1.000000\n",
            "distance     0.550000\n",
            "Name: 507, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "numerical_cols=[\"Age\", \"Fare\", \"SibSp\", \"Parch\"]\n",
        "categorical_cols=[\"Sex\", \"Embarked\", \"Pclass\", \"Deck\"]\n",
        "# Compute distances for survivors\n",
        "survivors[\"distance\"]=np.linalg.norm(\n",
        "    survivors[numerical_cols]-average_survivor[numerical_cols], axis=1)\n",
        "\n",
        "closest_survivor=survivors.loc[survivors[\"distance\"].idxmin()]\n",
        "print(\"Closest passenger to average survivor:\\n\", closest_survivor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closest passenger to non_survivor Survived     0.000000\n",
            "Pclass       2.000000\n",
            "Sex          1.000000\n",
            "Age         30.000000\n",
            "SibSp        0.000000\n",
            "Parch        0.000000\n",
            "Fare        10.500000\n",
            "Embarked     2.000000\n",
            "Deck        -1.000000\n",
            "distance     0.300882\n",
            "Name: 219, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "non_survivors[\"distance\"]=np.linalg.norm(\n",
        "    non_survivors[numerical_cols]-average_non_survivor[numerical_cols], axis=1\n",
        ")\n",
        "\n",
        "closest_non_survivor=non_survivors.loc[non_survivors[\"distance\"].idxmin()]\n",
        "print(\"Closest passenger to non_survivor\", closest_non_survivor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc96MC9CpYkM"
      },
      "source": [
        "4. Next, let's continue the analysis by looking into pairwise and multivariate relationships between the variables in the two groups. Try to visualize two variables at a time using, e.g., scatter plots and use a different color to encode the survival status.\n",
        "\n",
        "    <span style=\"font-weight: 500\">*Hint 1: You can also check out Seaborn's pairplot function, if you wish.*</span>\n",
        "\n",
        "    <span style=\"font-weight: 500\">*Hint 2: To better show many data points with the same value for a given variable, you can use either transparency or ‘jitter’.*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fexBqTMQpYkM"
      },
      "outputs": [],
      "source": [
        "# Use this cell for your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNOrCjKzpYkM"
      },
      "source": [
        "5. Finally, recall the preprocessing we did in the first exercise. What can you say about the effect of the choices that were made to use the mode and mean to impute missing values, instead of, for example, ignoring passengers with missing data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8TeRhWapYkM"
      },
      "source": [
        "*Use this (markdown) cell for your written answer*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsrhB5wvpYkM"
      },
      "source": [
        "**Remember to submit your code on the MOOC platform. You can return this Jupyter notebook (.ipynb) or .py, .R, etc depending on your programming preferences.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqgXjyklpYkM"
      },
      "source": [
        "## Exercise 3 | Working with text data 2.0\n",
        "\n",
        "This exercise is related to the second exercise from last week. Find the saved <span style=\"font-weight: 500\">pos.txt</span> and <span style=\"font-weight: 500\">neg.txt</span> files, or, alternatively, you can find the week 1 example solutions on the MOOC platform after Tuesday."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-DfVlSWpYkM"
      },
      "source": [
        "1. Find the most common words in each file (positive and negative). Examine the results. Do they tend to be general terms relating to the nature of the data? How well do they indicate positive/negative sentiment?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GDsK6jXMpYkM"
      },
      "outputs": [],
      "source": [
        "# Use this cell for your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlU_trsrpYkM"
      },
      "source": [
        "2. Compute a [TF/IDF](https://en.wikipedia.org/wiki/Tf–idf) vector for each of the two text files, and make them into a <span style=\"font-weight: 500\">2 x m</span> matrix, where <span style=\"font-weight: 500\">m</span> is the number of unique words in the data. The problem with using the most common words in a review to analyze its contents is that words that are common overall will be common in all reviews (both positive and negative). This means that they probably are not good indicators about the sentiment of a specific review. TF/IDF stands for Term Frequency / Inverse Document Frequency (here the reviews are the documents), and is designed to help by taking into consideration not just the number of times a term occurs (term frequency), but also how many times a word exists in other reviews as well (inverse document frequency). You can use any variant of the formula, as well as off-the-shelf implementations. <span style=\"font-weight: 500\">*Hint: You can use [sklearn](http://scikit-learn.org/).*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Tt_t-Lx8pYkM"
      },
      "outputs": [],
      "source": [
        "# Use this cell for your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lsOuLeOpYkM"
      },
      "source": [
        "3. List the words with the highest TF/IDF score in each class (positive | negative), and compare them to the most common words. What do you notice? Did TF/IDF work as expected?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "h1tkcbH5pYkM"
      },
      "outputs": [],
      "source": [
        "# Use this cell for your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z_vWhnXpYkM"
      },
      "source": [
        "4. Plot the words in each class with their corresponding TF/IDF scores. Note that there will be a lot of words, so you’ll have to think carefully to make your chart clear! If you can’t plot them all, plot a subset – think about how you should choose this subset.\n",
        "\n",
        "    <span style=\"font-weight: 500\">*Hint: you can use word clouds. But feel free to challenge yourselves to think of any other meaningful way to visualize this information!*</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5eUJ3HxlpYkN"
      },
      "outputs": [],
      "source": [
        "# Use this cell for your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8-yzV5HpYkN"
      },
      "source": [
        "**Remember to submit your code on the MOOC platform. You can return this Jupyter notebook (.ipynb) or .py, .R, etc depending on your programming preferences.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_Xd1n98pYkQ"
      },
      "source": [
        "## Exercise 4 | Junk charts\n",
        "\n",
        "There’s a thriving community of chart enthusiasts who keep looking for statistical graphics that they find inappropriate, and which they call “junk charts”, and who often also propose ways to improve them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w78AogNWpYkQ"
      },
      "source": [
        "1. Find at least three statistical visualizations you think are not very good and identify their problems. Copying examples from various junk chart websites is not accepted – you should find your own junk charts, out in the wild. You should be able to find good (or rather, bad) examples quite easily since a significant fraction of charts can have at least *some* issues. The examples you choose should also have different problems, e.g., try to avoid collecting three bar charts, all with problematic axes. Instead, try to find as interesting and diverse examples as you can."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJfbaQH2pYkQ"
      },
      "source": [
        "2. Try to produce improved versions of the charts you selected. The data is of course often not available, but perhaps you can try to extract it, at least approximately, from the chart. Or perhaps you can simulate data that looks similar enough to make the point.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-rwa7IrpYkQ"
      },
      "source": [
        "**Submit a PDF with all the charts (the ones you found and the ones you produced).**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
